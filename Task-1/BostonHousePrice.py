# -*- coding: utf-8 -*-
"""BostonHousePrice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n-vjhPOUq-xG9QmaMl7chE_VlPixwKZL
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import BaggingRegressor

df = pd.read_csv('/content/HousingData.csv')

df.head()

df.tail()

# Summary of dataset
df.describe()

df.corr()

df.cov()

df.info()

df.isnull().sum()

# FILL THE null values
df['CRIM'] = df['CRIM'].fillna(df['CRIM'].mean())
df['ZN'] = df['ZN'].fillna(df['ZN'].mean())
df['CHAS'] = df['CHAS'].fillna(df['CHAS'].mean())
df['AGE'] = df['AGE'].fillna(df['AGE'].mean())
df['LSTAT'] = df['LSTAT'].fillna(df['LSTAT'].mean())
df['INDUS'] = df['INDUS'].fillna(df['INDUS'].mean())

df.isnull().sum()

"""# EDA"""

# Create a boxplot
fig,ax = plt.subplots(ncols = 7,nrows = 2,figsize = (20,10))
index = 0
ax = ax.flatten()
for col,value in df.items():
  sns.boxplot(y = col,data = df,ax = ax[index])
  index +=1
plt.tight_layout(pad = 0.5,w_pad = 0.7,h_pad = 5.0)

# create dist plot
fig,ax = plt.subplots(ncols = 7,nrows = 2,figsize = (20,10))
index = 0
ax = ax.flatten()
for col,value in df.items():
  sns.distplot(value,ax = ax[index])
  index +=1
plt.tight_layout(pad = 0.5,w_pad = 0.7,h_pad = 5.0)

"""# min max normalization"""

cols = ['CRIM','ZN','TAX' ,'LSTAT']
for col in cols:
  mimimun = min(df[col])
  maximum = max(df[col])
  df[col] = (df[col] - mimimun)/(maximum - mimimun)

# create dist plot
fig,ax = plt.subplots(ncols = 7,nrows = 2,figsize = (20,10))
index = 0
ax = ax.flatten()
for col,value in df.items():
  sns.distplot(value,ax = ax[index])
  index +=1
plt.tight_layout(pad = 0.5,w_pad = 0.7,h_pad = 5.0)

"""# *Standardization*"""

scaler = preprocessing.StandardScaler()

# Fitting the data
scaled_cols = scaler.fit_transform(df[cols])
scaled_cols = pd.DataFrame(scaled_cols,columns = cols)
scaled_cols.head()

for col in cols:
  df[col] = scaled_cols[col]

# create dist plot
fig,ax = plt.subplots(ncols = 7,nrows = 2,figsize = (20,10))
index = 0
ax = ax.flatten()
for col,value in df.items():
  sns.distplot(value,ax = ax[index])
  index +=1
plt.tight_layout(pad = 0.5,w_pad = 0.7,h_pad = 5.0)

"""# Correlation matrix"""

corr = df.corr()
plt.figure.figsize = (10,10)
sns.heatmap(corr,annot = True,cmap = 'coolwarm')

sns.regplot(y=df['MEDV'],x = df['RM'])

sns.regplot(y = df['MEDV'],x = df['LSTAT'])

"""# Train Test Split"""

X = df.drop(columns = ['MEDV','RAD'], axis = 1)
y = df['MEDV']

"""# Model Training"""

def train(X, y, model):
    # split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2529)
    model.fit(X_train, y_train)

    # prediction
    y_pred = model.predict(X_test)
    cv_score = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5)
    cv_score = np.abs(np.mean(cv_score))

    print("Model Report")
    print("MSE:", mean_squared_error(y_test, y_pred))
    print("CV Score:", cv_score)

model = LinearRegression()
train(X, y, model)
coef = pd.Series(model.coef_, index=X.columns).sort_values()
coef.plot(kind='bar', title='Model Coefficients')

model = DecisionTreeRegressor()
train(X, y, model)
coef = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
coef.plot(kind='bar', title='Feature Importance')

model = RandomForestRegressor()
train(X, y, model)
coef = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
coef.plot(kind='bar', title='Feature Importance')

model = xgb.XGBRegressor()
train(X, y, model)
coef = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
coef.plot(kind='bar', title='Feature Importance')

model = ExtraTreesRegressor()
train(X, y, model)
coef = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
coef.plot(kind='bar', title='Feature Importance')

